{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zfJzQLoW5nTr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daycardoso/reinforcement-learning/blob/main/01_MAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-armed bandits"
      ],
      "metadata": {
        "id": "L7xahTrN37lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imports"
      ],
      "metadata": {
        "id": "zfJzQLoW5nTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KIuQoIgT5pU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código-base do caça-níquel\n",
        "\n",
        "A classe implementa um multiarmed bandit cuja recompensa média de cada braço é sorteada. Além disso, quando o braço é puxado (get_reward), a recompensa retornada é sorteada com uma gaussiana."
      ],
      "metadata": {
        "id": "rc-gcB5b4Eyy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6yXyBLF34Mc"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, k_arm=10, base_reward=0., seed=None):\n",
        "        self.k = k_arm\n",
        "        self.base_reward = base_reward\n",
        "        self.seed = seed\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        # Real reward for each action\n",
        "        self.q_true = np.random.randn(self.k) + self.base_reward\n",
        "        self.best_action = np.argmax(self.q_true)\n",
        "\n",
        "    def get_reward(self, action):\n",
        "        # Generate the reward under N(real reward, 1)\n",
        "        return np.random.randn() + self.q_true[action]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agentes\n",
        "\n",
        "Cada agente representará uma política"
      ],
      "metadata": {
        "id": "w_sPHooV5Ze-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classe abstrata do agente, representando a política\n",
        "\n",
        "Você implementará cada política em uma subclasse diferente"
      ],
      "metadata": {
        "id": "VODlQ5z851-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(ABC):\n",
        "    def __init__(self, k_arm=10, initial_estimate=0.):\n",
        "        self.k = k_arm\n",
        "        self.initial = initial_estimate\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Estimation for each action\n",
        "        self.q_estimation = np.zeros(self.k) + self.initial\n",
        "\n",
        "    @abstractmethod\n",
        "    def act(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def update(self, action, reward):\n",
        "        pass"
      ],
      "metadata": {
        "id": "GVaa4ENo5hpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Epsilon-greedy com estimativa pela média amostral\n",
        "\n",
        "Implemente o agente epsilon-greedy que atualiza as estimativas de valor de uma ação pela média amostral (soma das recompensas recebidas naquela ação / #de tentativas naquela ação)"
      ],
      "metadata": {
        "id": "vLDJLYuD4Qq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedySampleAverageAgent(Agent):\n",
        "    def __init__(self, k_arm=10, epsilon=0.1, initial_estimate=0):\n",
        "        super().__init__(k_arm, initial_estimate)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def act(self):\n",
        "        # escreva aqui seu codigo para a escolha de ação epsilon-gulosa\n",
        "        pass\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # escreva aqui seu codigo para atualização do valor da ação escolhida\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "xPnFfxyi6A5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Epsilon-greedy com atualização de passo constante"
      ],
      "metadata": {
        "id": "BUhV-PFq6L8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyConstantStepsizeAgent(Agent):\n",
        "    def __init__(self, k_arm=10, epsilon=0.1, initial_estimate=0., step_size=0.1):\n",
        "        super().__init__(k_arm, initial_estimate)\n",
        "        self.epsilon = epsilon\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def act(self):\n",
        "        # escreva aqui seu codigo para a escolha de ação epsilon-gulosa\n",
        "        pass\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # escreva aqui seu codigo para atualização do valor da ação escolhida\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "286KCWvD6KYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agente com atualização por subida de gradiente"
      ],
      "metadata": {
        "id": "8bcTA5tP6fS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientAgent(Agent):\n",
        "    def __init__(self, k_arm=10, step_size=0.1, initial_estimate=0.):\n",
        "        super().__init__(k_arm, initial_estimate)\n",
        "        self.step_size = step_size\n",
        "\n",
        "    def act(self):\n",
        "        # escreva aqui seu codigo para a escolha de ação softmax\n",
        "        pass\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # escreva aqui seu codigo para atualização do valor da ação escolhida\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "B_BbL3036i5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código para executar uma simulação\n",
        "\n",
        "Para usar, você deve informar quantas execuções independentes fará (runs), quantas iterações/tentativas haverá em cada execução (time), a lista de agentes (agents) e o multi-armed bandit (environment). A simulação retorna um vetor 2D (agentes x iterações) contendo a média de vezes que a melhor ação foi selecionada em cada iteração e outro vetor 2D (agentes x iterações) de recompensas médias por iteração."
      ],
      "metadata": {
        "id": "GDbn-Eri6nKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate(runs, time, agents, environment):\n",
        "    rewards = np.zeros((len(agents), runs, time))\n",
        "    best_action_counts = np.zeros(rewards.shape)\n",
        "    for i, agent in enumerate(agents):\n",
        "        for r in trange(runs):\n",
        "            environment.reset()\n",
        "            agent.reset()\n",
        "            for t in range(time):\n",
        "                action = agent.act()\n",
        "                reward = environment.get_reward(action)\n",
        "                agent.update(action, reward)\n",
        "                rewards[i, r, t] = reward\n",
        "                if action == environment.best_action:\n",
        "                    best_action_counts[i, r, t] = 1\n",
        "    mean_best_action_counts = best_action_counts.mean(axis=1)\n",
        "    mean_rewards = rewards.mean(axis=1)\n",
        "    return mean_best_action_counts, mean_rewards"
      ],
      "metadata": {
        "id": "qnfLWeuI6o4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de simulação, gerando um gráfico"
      ],
      "metadata": {
        "id": "vyzi7ufW7G9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def example_simulation(runs=50, time=500):\n",
        "    env = Environment(k_arm=10)\n",
        "    agents = [EpsilonGreedySampleAverageAgent(epsilon=0.1, initial_estimate=0)]\n",
        "\n",
        "    best_action_counts, rewards = simulate(runs, time, agents, env)\n",
        "\n",
        "    plt.plot(rewards[0], label='$\\epsilon = .1$')\n",
        "    plt.xlabel('steps')\n",
        "    plt.ylabel('average reward')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "example_simulation()"
      ],
      "metadata": {
        "id": "4rFBDqHB7Kiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tarefas\n",
        "\n",
        "1. Implementar as subclasses dos agentes\n",
        "2. Em um multi-armed bandit com 10 alavancas e recompensa base de 0, gerar um gráfico comparando as políticas a seguir. Cada política deve ser executada 2000 vezes com 1000 tentativas em cada vez.\n",
        "  * epsilon-greedy atualizado com média amostral com epsilon=0\n",
        "  * epsilon-greedy atualizado com média amostral com epsilon=0.1\n",
        "  * epsilon-greedy atualizado com média amostral com epsilon=0.01\n",
        "3. Em um multi-armed bandit com 10 alavancas e recompensa base de 0, gerar um gráfico comparando as políticas a seguir (use alpha=0.1 para os agentes que atualizam estimativas usando tamanho de passo constante). Cada política deve ser executada 2000 vezes com 1000 tentativas em cada vez.\n",
        "  * epsilon-greedy atualizado com média amostral com epsilon=0.01\n",
        "  * epsilon-greedy atualizado com tamanho de passo constante com epsilon=0.01\n",
        "  * gradiente\n",
        "4. Crie uma nova classe modificando Environment, de forma a criar um multi-armed bandit não-estacionário (que muda ao longo do tempo). Em especial, faça os q_true iniciarem iguais (e.g. 0) e, a cada passo (step), cada um muda sendo somado a um valor sorteado por uma Normal(0,0.01). Reexecute a comparação da Tarefa 3, porém com 1000 repetições de 10 mil tentativas para cada agente."
      ],
      "metadata": {
        "id": "GMNxwB2PBFhS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-1zjVeL9tM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}